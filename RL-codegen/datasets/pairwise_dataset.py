#
# Copyright (c) 2022, salesforce.com, inc.
# All rights reserved.
# SPDX-License-Identifier: BSD-3-Clause
# For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause
#

import torch
import glob
import logging
import random
import fnmatch
import numpy as np
import gc
import os
from tqdm import tqdm
from collections import Counter
import pickle as pkl
import json, pdb

from multiprocessing import Manager
import transformers

import datasets.utils as dsutils


class APPSPairwiseDataset(torch.utils.data.Dataset):
    '''
    This dataset loads pairs of code solutions generated by a LM on the same problem, with different outputs.
    Based on the outputs of the two programs, one will be ranked higher than the other
    '''
    def __init__(self, dataroot, problem_dirs, model, max_tokens, sample_mode,
                 tuning_mode, max_src_tokens, relative_returns, returns=[-1, -0.6, -0.3, 1.0]):
        self.dataroot = dataroot
        self.problem_dirs = problem_dirs
        self.sample_mode = sample_mode
        self.model = model
        self.returns = returns

        self.max_tokens = max_tokens
        self.max_src_tokens = max_src_tokens

        self.all_error_types, self.all_error_subtypes, self.all_baseline_error_types = [], [], []
        self.initialize()

        if self.model in ['codet5-base', 'codet5-large']:
            self.tokenizer = transformers.RobertaTokenizer.from_pretrained('Salesforce/codet5-base')

    def load_sample_pairs(self, sols, answer_type, starter_code, question_str, gt_solution):
        '''
        Create Pairwise comparisons between generated solutions (possibly use expert demonstrations as well)
        '''
        samples = []
        info = []
        result_conversion = {
            -2: -2,
            -1: -1,
            False: 0,
            True: 1
        }
        for idx, sol in enumerate(sols):
            sol_str = sol['code']
            result = sol['result']
            error_type = sol['error_type']

            # Sample random comparison
            for idx2 in range(idx + 1, len(sols) + 1):
                if idx2 == len(sols):
                    sol_str2 = gt_solution
                    result2 = True
                    error_type2 = "passed unit tests"
                else:
                    sol_str2 = sols[idx2]['code']
                    result2 = sols[idx2]['result']
                    error_type2 = sols[idx2]['error_type']

                if result2 == result:
                    continue
                else:
                    result_num1 = result_conversion[result]
                    result_num2 = result_conversion[result2]
                    if result_num1 < result_num2:
                        sample = (question_str, starter_code, sol_str, sol_str2, answer_type)
                    else:
                        sample = (question_str, starter_code, sol_str2, sol_str, answer_type)

                    samples.append(sample)
                    info.append((result, result2))

        return samples, info

    def load_gt_samples(self, sols, answer_type, starter_code, question_str):
        samples = []

        for sol_str in sols:
            sol_str = dsutils.reindent_code(sol_str)
            sample = (question_str, starter_code, sol_str, answer_type)
            samples.append(sample)

        return samples

    def get_gt_info(self):
        return (1, None)

    def get_baseline_error_type(self, sols):
        return dsutils.get_error_type(sols[0]['result'])

    def update_error_stat(self, info):
        for i in info:
            error_type = dsutils.get_error_type(i[0])
            error_subtype = i[1]
            self.all_error_types.append(error_type)
            self.all_error_subtypes.append(error_subtype)

    def update_error_stat_rl(self, info):
        for i in info:
            error_type = i[0]
            baseline_error_type = i[-1]
            self.all_error_types.append(error_type)
            self.all_baseline_error_types.append(baseline_error_type)

    def initialize(self):
        skipped_problems = []
        samples_info = []
        gen_samples = []
        num_samples = 0
        print(f"Loading {len(self.problem_dirs)} problems from {self.dataroot}.")
        for problem_name in tqdm(self.problem_dirs):
            gen_sols_fname = [os.path.join(self.dataroot, problem_name, "gen_solutions.json")]

            question_fname = os.path.join(self.dataroot, problem_name, "question.txt")
            sols_fname = os.path.join(self.dataroot, problem_name, "solutions.json")
            if (not os.path.isfile(question_fname)) or (not os.path.isfile(sols_fname)):
                skipped_problems.append(problem_name)
                continue
            num_samples += 1
            # Read the question description
            with open(question_fname, 'r') as f:
                question_str = f.read()

            starter_code = os.path.join(self.dataroot, problem_name, "starter_code.py")
            if (os.path.isfile(starter_code)):
                answer_type = "\nUse Call-Based format\n"
                with open(starter_code, 'r') as f:
                    starter_code = f.read()
            else:
                answer_type = "\nUse Standard Input format\n"
                starter_code = ""

            sols_str_list = json.load(open(sols_fname, 'r'))
            gt_samples = self.load_gt_samples(sols_str_list, answer_type, starter_code, question_str)
            gt_question_str, gt_starter_code, gt_sol_str, gt_answer_type = gt_samples[0] # Try to change this later

            # Read all the solutions
            for fname in gen_sols_fname:
                if os.path.exists(fname):
                    gen_sols = json.load(open(fname, 'r'))
                    samples, info = self.load_sample_pairs(gen_sols, answer_type, starter_code, question_str, gt_sol_str)
                    # self.update_error_stat(info)
                    gen_samples += samples
                    samples_info += info

        print(f"Loaded {num_samples} samples from {self.dataroot}.")
        print(f"Skipped {len(skipped_problems)} problems from {self.dataroot}.")


        self.samples_info = samples_info
        self.gen_samples = gen_samples

    def __len__(self):
        return len(self.gen_samples)

    def __getitem__(self, idx):

        raw_samples = self.pack_samples(idx, 'gen')
        inputs = self.sample_task(raw_samples, 'gen')

        gc.collect()
        return inputs

    def pack_samples(self, idx, sample_type=None):
        """
        Repeatedly pick question, answer pairs from self.dataroot until we hit max_tokens.
        This will not include the tokens for the QUESTION and ANSWER prompt, as well as the
        self.question_prefix. These will be added later and the total input will be
        truncated if necessary.

        Always include the sample at idx at the beginning.
        """
        curr_num_tokens = 0
        curr_samples = []

        sample_pool = self.gen_samples

        # sample = (question_str, starter_code, sol_str, sol_str2, answer_type)
        if self.sample_mode == 'uniform_sol':
            curr_q, curr_s, curr_a1, curr_a2, curr_q_prefix = sample_pool[idx]

        elif self.sample_mode == 'uniform_prob':
            raise NotImplementedError()

        while curr_num_tokens < self.max_tokens:

            curr_q = curr_q[:150000]
            curr_s = curr_s[:150000]
            curr_a1 = curr_a1[:150000]
            curr_a2 = curr_a2[:150000]

            curr_num_tokens += len(self.tokenizer.tokenize(curr_q))
            curr_num_tokens += len(self.tokenizer.tokenize(curr_s))
            curr_num_tokens += len(self.tokenizer.tokenize(curr_a1))
            curr_num_tokens += len(self.tokenizer.tokenize(curr_a2))

            curr_samples.append((curr_q, curr_s, curr_a1, curr_a2, curr_q_prefix))
            break

        return curr_samples

    def sample_task(self, samples, sample_type=None):

        input_ids = []
        label_ids1 = []
        label_ids2 = []


        for sample in samples:
            # curr_samples.append((curr_q, curr_s, curr_a1, curr_a2, curr_type))
            q_str, s_str, a_str1, a_str2, answer_type = sample

            q_str = "\nQUESTION:\n" + q_str + "\n" + s_str + "\n" + answer_type + "\nANSWER:\n"

            question_token_ids = self.tokenizer.encode(q_str, verbose=False)
            input_ids.extend(question_token_ids)

            answer_token_ids1 = self.tokenizer.encode(a_str1, verbose=False)
            answer_token_ids2 = self.tokenizer.encode(a_str2, verbose=False)
            # if self.model not in ['codet5-base', 'codet5-large']:
            #     label_ids.extend([-100] * len(question_token_ids))
            #     answer_token_ids.append(self.tokenizer.eos_token_id)
            #     input_ids.extend(answer_token_ids)

            label_ids1.extend(answer_token_ids1)
            label_ids2.extend(answer_token_ids2)


        # Sanity checks and padding
        input_ids_max_len = self.max_src_tokens if self.model in ['codet5-base', 'codet5-large'] else self.max_tokens
        if len(input_ids) < input_ids_max_len:
            new_input_ids = [self.tokenizer.eos_token_id] * input_ids_max_len
            new_input_ids[:len(input_ids)] = input_ids
            input_ids = new_input_ids

            if self.model not in ['codet5-base', 'codet5-large']:
                new_label_ids = [-100] * input_ids_max_len
                new_label_ids[:len(label_ids1)] = label_ids1
                label_ids1 = new_label_ids

                new_label_ids = [-100] * input_ids_max_len
                new_label_ids[:len(label_ids2)] = label_ids2
                label_ids2 = new_label_ids


        if self.model in ['codet5-base', 'codet5-large'] and len(label_ids1) < self.max_tokens:
            new_label_ids = [-100] * self.max_tokens
            new_label_ids[:len(label_ids1)] = label_ids1
            label_ids1 = new_label_ids

        if self.model in ['codet5-base', 'codet5-large'] and len(label_ids2) < self.max_tokens:
            new_label_ids = [-100] * self.max_tokens
            new_label_ids[:len(label_ids2)] = label_ids2
            label_ids2 = new_label_ids

        if self.model not in ['codet5-base', 'codet5-large'] and len(input_ids) != len(label_ids1): pdb.set_trace()


        # Cut off the excess
        input_ids = input_ids[:input_ids_max_len]
        label_ids1 = label_ids1[:self.max_tokens]
        label_ids2 = label_ids2[:self.max_tokens]

        out_sample = {
            "input_ids": torch.LongTensor(input_ids),
            "labels1": torch.LongTensor(label_ids1),
            "labels2": torch.LongTensor(label_ids2),
        }

        return out_sample

